{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-ead3e842d5b7>, line 109)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-ead3e842d5b7>\"\u001b[1;36m, line \u001b[1;32m109\u001b[0m\n\u001b[1;33m    gaussian = cv2.GaussianBlur(img, (9,9), 10.0) #unblur\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import os.path as path\n",
    "print(os.listdir(\"../input\"))\n",
    "import glob\n",
    "import cv2\n",
    "import pickle\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D, Activation,Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as k\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "\n",
    "FIG_WIDTH=20 # width of figure\n",
    "HEIGHT_PER_ROW=3 # height of each row when showing a figure which consists of multiple rows\n",
    "RESIZE_DIM=32 # images will be resized to 32x32 pixels\n",
    "MODEL_NAME = 'bangla_digit_convnet_new'\n",
    "\n",
    "def load_data():\n",
    "    data_dir=os.path.join('..','input')\n",
    "    paths_train_a=glob.glob(os.path.join(data_dir,'training-a','*.png'))\n",
    "    paths_train_b=glob.glob(os.path.join(data_dir,'training-b','*.png'))\n",
    "    paths_train_e=glob.glob(os.path.join(data_dir,'training-e','*.png'))\n",
    "    paths_train_c=glob.glob(os.path.join(data_dir,'training-c','*.png'))\n",
    "    paths_train_d=glob.glob(os.path.join(data_dir,'training-d','*.png'))\n",
    "    paths_train_all=paths_train_a+paths_train_b+paths_train_c+paths_train_d+paths_train_e\n",
    "\n",
    "    paths_test_a=glob.glob(os.path.join(data_dir,'testing-a','*.png'))\n",
    "    paths_test_b=glob.glob(os.path.join(data_dir,'testing-b','*.png'))\n",
    "    paths_test_e=glob.glob(os.path.join(data_dir,'testing-e','*.png'))\n",
    "    paths_test_c=glob.glob(os.path.join(data_dir,'testing-c','*.png'))\n",
    "    paths_test_d=glob.glob(os.path.join(data_dir,'testing-d','*.png'))\n",
    "    paths_test_f=glob.glob(os.path.join(data_dir,'testing-f','*.png'))+glob.glob(os.path.join(data_dir,'testing-f','*.JPG'))\n",
    "    paths_test_auga=glob.glob(os.path.join(data_dir,'testing-auga','*.png'))\n",
    "    paths_test_augc=glob.glob(os.path.join(data_dir,'testing-augc','*.png'))\n",
    "    paths_test_all=paths_test_a+paths_test_b+paths_test_c+paths_test_d+paths_test_e+paths_test_f+paths_test_auga+paths_test_augc\n",
    "    \n",
    "    path_label_train_a=os.path.join(data_dir,'training-a.csv')\n",
    "    path_label_train_b=os.path.join(data_dir,'training-b.csv')\n",
    "    path_label_train_e=os.path.join(data_dir,'training-e.csv')\n",
    "    path_label_train_c=os.path.join(data_dir,'training-c.csv')\n",
    "    path_label_train_d=os.path.join(data_dir,'training-d.csv')\n",
    "    \n",
    "    X_train_a,y_train_a=get_data(paths_train_a,path_label_train_a,resize_dim=RESIZE_DIM)\n",
    "    X_train_b,y_train_b=get_data(paths_train_b,path_label_train_b,resize_dim=RESIZE_DIM)\n",
    "    X_train_c,y_train_c=get_data(paths_train_c,path_label_train_c,resize_dim=RESIZE_DIM)\n",
    "    X_train_d,y_train_d=get_data(paths_train_d,path_label_train_d,resize_dim=RESIZE_DIM)\n",
    "    X_train_e,y_train_e=get_data(paths_train_e,path_label_train_e,resize_dim=RESIZE_DIM)\n",
    "    \n",
    "    X_train_all=np.concatenate((X_train_a,X_train_b,X_train_c,X_train_d,X_train_e),axis=0)\n",
    "    y_train_all=np.concatenate((y_train_a,y_train_b,y_train_c,y_train_d,y_train_e),axis=0)\n",
    "    \n",
    "    X_show_all=X_train_all\n",
    "    \n",
    "    X_test_a=get_data(paths_test_a,resize_dim=RESIZE_DIM)\n",
    "    X_test_b=get_data(paths_test_b,resize_dim=RESIZE_DIM)\n",
    "    X_test_c=get_data(paths_test_c,resize_dim=RESIZE_DIM)\n",
    "    X_test_d=get_data(paths_test_d,resize_dim=RESIZE_DIM)\n",
    "    X_test_e=get_data(paths_test_e,resize_dim=RESIZE_DIM)\n",
    "    X_test_f=get_data(paths_test_f,resize_dim=RESIZE_DIM)\n",
    "    X_test_auga=get_data(paths_test_auga,resize_dim=RESIZE_DIM)\n",
    "    X_test_augc=get_data(paths_test_augc,resize_dim=RESIZE_DIM)\n",
    "    \n",
    "    X_test_all=np.concatenate((X_test_a,X_test_b,X_test_c,X_test_d,X_test_e,X_test_f,X_test_auga,X_test_augc))\n",
    "    \n",
    "    X_tshow_all=X_test_all\n",
    "    \n",
    "    X_train_all = X_train_all.reshape(X_train_all.shape[0],32, 32,1).astype('float32')\n",
    "    X_test_all = X_test_all.reshape(X_test_all.shape[0],32, 32,1).astype('float32')\n",
    "    \n",
    "    X_train_all = X_train_all/255\n",
    "    X_test_all=X_test_all/255\n",
    "    \n",
    "    indices=list(range(len(X_train_all)))\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    ind=int(len(indices)*0.80)\n",
    "    # train data\n",
    "    X_train=X_train_all[indices[:ind]] \n",
    "    y_train=y_train_all[indices[:ind]]\n",
    "    # validation data\n",
    "    X_val=X_train_all[indices[-(len(indices)-ind):]] \n",
    "    y_val=y_train_all[indices[-(len(indices)-ind):]]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n",
    "    \n",
    "def get_key(path):\n",
    "    # seperates the key of an image from the filepath\n",
    "    key=path.split(sep=os.sep)[-1]\n",
    "    return key\n",
    "    \n",
    "def get_data(paths_img,path_label=None,resize_dim=None):\n",
    "    X=[] # initialize empty list for resized images\n",
    "    for i,path in enumerate(paths_img):\n",
    "        img=cv2.imread(path,cv2.IMREAD_GRAYSCALE) # images loaded in color (BGR)\n",
    "        if resize_dim is not None:\n",
    "            img=cv2.resize(img,(resize_dim,resize_dim),interpolation=cv2.INTER_AREA\n",
    "        \n",
    "        gaussian = cv2.GaussianBlur(img, (9,9), 10.0) #unblur\n",
    "        img = cv2.addWeighted(img, 1.5, gaussian, -0.5, 0, img)\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) #filter\n",
    "        img = cv2.filter2D(img, -1, kernel)\n",
    "        ret,img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        X.append(img)\n",
    "        \n",
    "    X=np.array(X) # tranform list to numpy array\n",
    "    if  path_label is None:\n",
    "        return X\n",
    "    else:\n",
    "        df = pd.read_csv(path_label) # read labels\n",
    "        df=df.set_index('filename') \n",
    "        y_label=[df.loc[get_key(path)]['digit'] for path in  paths_img] # get the labels corresponding to the images\n",
    "        y=to_categorical(y_label,10) # transfrom integer value to categorical variable\n",
    "        return X, y\n",
    "    \n",
    "def my_model(img_size,channels):\n",
    "    model = Sequential()\n",
    "    input_shape = (img_size,img_size,channels)\n",
    "    model.add(Conv2D(32, (5, 5), input_shape=input_shape,activation='relu', padding='same'))\n",
    "    model.add(Conv2D(32, (5, 5), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3),activation='relu',padding='same'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(Conv2D(256, (3, 3),activation='relu',padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer='adam')\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(X_train,y_train,X_val,y_val):\n",
    "    path_model='model_filter.h5' # location of saving model after each epoch\n",
    "    K.tensorflow_backend.clear_session() # destroys the current graph and builds a new one\n",
    "    model=my_model(32,1) # create the model\n",
    "    K.set_value(model.optimizer.lr,1e-3) # set the learning rate\n",
    "    # fit the model\n",
    "    h=model.fit(x=X_train,     \n",
    "                y=y_train, \n",
    "                batch_size=64, \n",
    "                epochs=30, \n",
    "                verbose=1, \n",
    "                validation_data=(X_val,y_val),\n",
    "                shuffle=True,\n",
    "                callbacks=[\n",
    "                    ModelCheckpoint(filepath=path_model),\n",
    "                ]\n",
    "                )\n",
    "                \n",
    "def export_model(saver, model, input_node_names, output_node_name):\n",
    "    tf.train.write_graph(K.get_session().graph_def, 'out', \\\n",
    "        MODEL_NAME + '_graph.pbtxt')\n",
    "\n",
    "    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')\n",
    "    \n",
    "    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \\\n",
    "        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \\\n",
    "        \"save/restore_all\", \"save/Const:0\", \\\n",
    "        'out/frozen_' + MODEL_NAME + '.pb', True, \"\")\n",
    "\n",
    "    input_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', \"rb\") as f:\n",
    "        input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "            input_graph_def, input_node_names, [output_node_name],\n",
    "            tf.float32.as_datatype_enum)\n",
    "\n",
    "    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', \"wb\") as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    print(\"graph saved!\")\n",
    "    \n",
    "\n",
    "def main():\n",
    "    if not path.exists('out'):\n",
    "        os.mkdir('out')\n",
    "\n",
    "    X_train, y_train, X_val, y_val = load_data()\n",
    "\n",
    "    train_model(X_train,y_train,X_val,y_val)\n",
    "    \n",
    "    model = my_model(32,1)\n",
    "\n",
    "    export_model(tf.train.Saver(), model, [\"conv2d_1_input\"], \"activation_2/Softmax\")\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()    \n",
    "                \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
